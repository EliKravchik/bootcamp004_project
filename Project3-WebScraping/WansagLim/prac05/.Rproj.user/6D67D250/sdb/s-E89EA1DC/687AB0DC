{
    "contents" : "######################################################################\n######################################################################\n#####[09] Trees, Bagging, Random Forests, & Boosting Lecture Code#####\n######################################################################\n######################################################################\n\n\n\n##############################\n#####Classification Trees#####\n##############################\n#Loading the tree library for fitting classification and regression trees.\nlibrary(tree)\n\n#Loading the ISLR library in order to use the Carseats dataset.\nlibrary(ISLR)\n\n#Making data manipulation easier.\nhelp(Carseats)\nattach(Carseats)\n\n#Looking at the variable of interest, Sales.\nhist(Sales)\nsummary(Sales)\n\n#Creating a binary categorical variable High based on the continuous Sales\n#variable and adding it to the original data frame.\nHigh = ifelse(Sales <= 8, \"No\", \"Yes\")\nCarseats = data.frame(Carseats, High)\n\n#Fit a tree to the data; note that we are excluding Sales from the formula.\ntree.carseats = tree(High ~ . - Sales, split = \"gini\", data = Carseats)\nsummary(tree.carseats)\n\n#The output shows the variables actually used within the tree, the number of\n#terminal nodes, the residual mean deviance based on the Gini index, and\n#the misclassification error rate.\n\n#Plotting the classification tree.\nplot(tree.carseats)\ntext(tree.carseats, pretty = 0) #Yields category names instead of dummy variables.\n\n#Detailed information for the splits of the classification tree.\ntree.carseats\n\n#The output shows the variables used at each node, the split rule, the number\n#of observations at each node, the deviance based on the Gini index, the\n#majority class value based on the observations in the node, and the associated\n#probabilities of class membership at each node. Terminal nodes are denoted\n#by asterisks.\n\n#Splitting the data into training and test sets by an 70% - 30% split.\nset.seed(0)\ntrain = sample(1:nrow(Carseats), 7*nrow(Carseats)/10) #Training indices.\nCarseats.test = Carseats[-train, ] #Test dataset.\nHigh.test = High[-train] #Test response.\n\n#Ftting and visualizing a classification tree to the training data.\ntree.carseats = tree(High ~ . - Sales, data = Carseats, subset = train)\nplot(tree.carseats)\ntext(tree.carseats, pretty = 0)\nsummary(tree.carseats)\ntree.carseats\n\n#Using the trained decision tree to classify the test data.\ntree.pred = predict(tree.carseats, Carseats.test, type = \"class\")\ntree.pred\n\n#Assessing the accuracy of the overall tree by constructing a confusion matrix.\ntable(tree.pred, High.test)\n(60 + 42)/120\n\n#Performing cross-validation in order to decide how many splits to prune; using\n#misclassification as the basis for pruning.\nset.seed(0)\ncv.carseats = cv.tree(tree.carseats, FUN = prune.misclass)\n\n#Inspecting the elements of the cv.tree() object.\nnames(cv.carseats)\ncv.carseats\n\n#Size indicates the number of terminal nodes. Deviance is the criterion we\n#specify; in this case it is the misclassification rate. K is analogous to the\n#cost complexity tuning parameter alpha. Method indicates the specified criterion.\n\n#Visually inspecting the results of the cross-validation by considering tree\n#complexity.\npar(mfrow = c(1, 2))\nplot(cv.carseats$size, cv.carseats$dev, type = \"b\",\n     xlab = \"Terminal Nodes\", ylab = \"Misclassified Observations\")\nplot(cv.carseats$k, cv.carseats$dev, type  = \"b\",\n     xlab = \"Alpha\", ylab = \"Misclassified Observations\")\n\n#Pruning the overall tree to have 4 terminal nodes; choose the best tree with\n#4 terminal nodes based on cost complexity pruning.\npar(mfrow = c(1, 1))\nprune.carseats = prune.misclass(tree.carseats, best = 4)\nplot(prune.carseats)\ntext(prune.carseats, pretty = 0)\n\n#Assessing the accuracy of the pruned tree with 4 terminal nodes by constructing\n#a confusion matrix.\ntree.pred = predict(prune.carseats, Carseats.test, type = \"class\")\ntable(tree.pred, High.test)\n(53 + 33)/120\n\n#Originally we had 25 terminal nodes and an accuracy of 85%; now, there are\n#only 4 terminal nodes with an accuracy of about 71.67%.\n\n\n\n##########################\n#####Regression Trees#####\n##########################\n#Inspecting the housing values in the suburbs of Boston.\nlibrary(MASS)\nhelp(Boston)\n\n#Creating a training set on 70% of the data.\nset.seed(0)\ntrain = sample(1:nrow(Boston), 7*nrow(Boston)/10)\n\n#Training the tree to predict the median value of owner-occupied homes (in $1k).\ntree.boston = tree(medv ~ ., Boston, subset = train)\nsummary(tree.boston)\n\n#Visually inspecting the regression tree.\nplot(tree.boston)\ntext(tree.boston, pretty = 0)\n\n#Performing cross-validation.\nset.seed(0)\ncv.boston = cv.tree(tree.boston)\npar(mfrow = c(1, 2))\nplot(cv.boston$size, cv.boston$dev, type = \"b\",\n     xlab = \"Terminal Nodes\", ylab = \"RSS\")\nplot(cv.boston$k, cv.boston$dev, type  = \"b\",\n     xlab = \"Alpha\", ylab = \"RSS\")\n\n#Pruning the tree to have 4 terminal nodes.\nprune.boston = prune.tree(tree.boston, best = 4)\npar(mfrow = c(1, 1))\nplot(prune.boston)\ntext(prune.boston, pretty = 0)\n\n#Calculating and assessing the MSE of the test data on the overall tree.\nyhat = predict(tree.boston, newdata = Boston[-train, ])\nyhat\nboston.test = Boston[-train, \"medv\"]\nboston.test\nplot(yhat, boston.test)\nabline(0, 1)\nmean((yhat - boston.test)^2)\n\n#Calculating and assessing the MSE of the test data on the pruned tree.\nyhat = predict(prune.boston, newdata = Boston[-train, ])\nyhat\nplot(yhat, boston.test)\nabline(0, 1)\nmean((yhat - boston.test)^2)\n\n\n\n##################################\n#####Bagging & Random Forests#####\n##################################\nlibrary(randomForest)\n\n#Fitting an initial random forest to the training subset.\nset.seed(0)\nrf.boston = randomForest(medv ~ ., data = Boston, subset = train, importance = TRUE)\nrf.boston\n\n#The MSE and percent variance explained are based on out-of-bag estimates,\n#yielding unbiased error estimates. The model reports that mtry = 4, which is\n#the number of variables randomly chosen at each split. Since we have 13 overall\n#variables, we could try all 13 possible values of mtry. We will do so, record\n#the results, and make a plot.\n\n#Varying the number of variables used at each step of the random forest procedure.\nset.seed(0)\noob.err = double(13)\ntest.err = double(13)\nfor (mtry in 1:13) {\n  fit = randomForest(medv ~ ., data = Boston, subset = train, mtry = mtry)\n  oob.err[mtry] = fit$mse[500]\n  pred = predict(fit, Boston[-train, ])\n  test.err[mtry] = with(Boston[-train, ], mean((medv - pred)^2))\n  cat(\"We're performing iteration\", mtry, \"\\n\")\n}\n\n#Visualizing the test error and the OOB error.\nmatplot(1:13, cbind(test.err, oob.err),\n        pch = 16, col = c(\"red\", \"blue\"), type = \"b\",\n        xlab = \"Variables at Each Split\", ylab = \"Mean Squared Error\")\nlegend(\"topright\", legend = c(\"Test\", \"OOB\"), pch = 19, col = c(\"red\", \"blue\"))\n\n#Can visualize a variable importance plot.\nimportance(rf.boston)\nvarImpPlot(rf.boston)\n\n\n\n##################\n#####Boosting#####\n##################\nlibrary(gbm)\n\n#Fitting 10,000 trees with a depth of 4.\nset.seed(0)\nboost.boston = gbm(medv ~ ., data = Boston[train, ],\n                   distribution = \"gaussian\",\n                   n.trees = 10000,\n                   interaction.depth = 4)\n\n#Inspecting the relative influence.\npar(mfrow = c(1, 1))\nsummary(boost.boston)\n\n#Partial dependence plots for specific variables; these plots illustrate the\n#marginal effect of the selected variables on the response after integrating\n#out the other variables.\npar(mfrow = c(1, 2))\nplot(boost.boston, i = \"rm\")\nplot(boost.boston, i = \"lstat\")\n\n#As the number of higher prooportion of lower status people in the suburb, the\n#lower the value of the housing prices. As the number of rooms increases, so\n#does the expected house price.\n\n#Letâ€™s make a prediction on the test set. With boosting, the number of trees is\n#a tuning parameter; having too many can cause overfitting. In general, we should\n#use cross validation to select the number of trees. Instead, we will compute the\n#test error as a function of the number of trees and make a plot for illustrative\n#purposes.\nn.trees = seq(from = 100, to = 10000, by = 100)\npredmat = predict(boost.boston, newdata = Boston[-train, ], n.trees = n.trees)\n\n#Produces 100 different predictions for each of the 152 observations in our\n#test set.\ndim(predmat)\n\n#Calculating the boosted errors.\npar(mfrow = c(1, 1))\nberr = with(Boston[-train, ], apply((predmat - medv)^2, 2, mean))\nplot(n.trees, berr, pch = 16,\n     ylab = \"Mean Squared Error\",\n     xlab = \"# Trees\",\n     main = \"Boosting Test Error\")\n\n#Include the best test error from the random forest.\nabline(h = min(test.err), col = \"red\")\n\n#Increasing the shrinkage parameter; a higher proportion of the errors are\n#carried over.\nset.seed(0)\nboost.boston2 = gbm(medv ~ ., data = Boston[train, ],\n                    distribution = \"gaussian\",\n                    n.trees = 10000,\n                    interaction.depth = 4,\n                    shrinkage = 0.1)\npredmat2 = predict(boost.boston2, newdata = Boston[-train, ], n.trees = n.trees)\n\nberr2 = with(Boston[-train, ], apply((predmat2 - medv)^2, 2, mean))\nplot(n.trees, berr2, pch = 16,\n     ylab = \"Mean Squared Error\",\n     xlab = \"# Trees\",\n     main = \"Boosting Test Error\")",
    "created" : 1456329879037.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3080918532",
    "id" : "687AB0DC",
    "lastKnownWriteTime" : 1456246301,
    "path" : "/Volumes/64GB/Documents/01DataScience/classMaterial/[09] Trees, Bagging, Random Forests, & Boosting Lecture Code.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 5,
    "source_on_save" : false,
    "type" : "r_source"
}